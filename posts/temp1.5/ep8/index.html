<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>【Temperature 1.5 的日常】EP8: 當 Structured Output 遇上評價指標 - 打造自動化的 LLM 裁判員 | Aura's Space</title><meta name=keywords content="LangChain,Python,Azure AI"><meta name=description content="

本文為個人學習筆記，記錄了學習過程中的一些知識，可參考，但不可認真，學習的過程可能有理解錯誤，資訊不一定正確，畢竟 Temperature 都 1.5 了。


零、 前言
接續上次我們聊到的 自定義中介軟體 (Custom Middleware)，我們掌握了攔截與修改 Agent 執行路徑的能力。但在 AI 應用落地時，最讓開發者頭痛的往往不是「怎麼跑」，而是「跑得好不好」。我們該如何量化一個 RAG（檢索增強生成）系統的表現？
今天我們要將 EP2 提到的結構化輸出 (Structured Output) 與 Azure AI 評價定義 結合。透過定義嚴謹的 Schema 與提示詞，讓 LLM 化身為公正的裁判，為每一次的對話進行精確打分。

一、 評價的三大支柱：Azure AI 評分定義
在 RAG 場景中，要衡量一個系統的品質，通常會參考以下三項關鍵指標。這些定義在 Azure AI 的評分框架中非常完整，我們可以將其直接注入到 Prompt 中：
1. Groundedness (誠實度/接地性) LINK
評估 RESPONSE 是否完全基於提供的 CONTEXT。

核心準則：Context 是唯一的真理。
評分級別：
1 分：完全無關。
2 分：嘗試回應但包含錯誤資訊。
3 分：正確但語意模糊（太過通泛）。
4 分：大部分正確，僅有輕微錯誤。
5 分：完整且精確（包含所有相關細節）。

2. Relevance (相關性) LINK
評估 RESPONSE 是否直接解決了使用者的 QUERY。

核心準則：是否回答了問題？有無洞察力？
評分級別：
1 分：離題、毫無關聯。
2 分：部分相關但未回答核心問題。
3 分：部分相關但資訊不足。
4 分：高度相關但缺乏深度。
5 分：全面且具備延伸見解。

3. Retrieval (檢索品質) LINK
評估被檢索出的 CONTEXT 塊是否真的對回答問題有幫助。"><meta name=author content="Aura"><link rel=canonical href=https://aura.codex.tw/posts/temp1.5/ep8/><link crossorigin=anonymous href=/assets/css/stylesheet.0a190288459736d4d10e13cb6ee8068b9aa282b3f8938264b3b2d6a98bf701b7.css integrity="sha256-ChkCiEWXNtTRDhPLbugGi5qigrP4k4Jks7LWqYv3Abc=" rel="preload stylesheet" as=style><link rel=icon href=https://aura.codex.tw/images/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://aura.codex.tw/images/favicon.ico><link rel=icon type=image/png sizes=32x32 href=https://aura.codex.tw/images/favicon.ico><link rel=apple-touch-icon href=https://aura.codex.tw/images/favicon.ico><link rel=mask-icon href=https://aura.codex.tw/images/favicon.ico><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://aura.codex.tw/posts/temp1.5/ep8/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://aura.codex.tw/posts/temp1.5/ep8/"><meta property="og:site_name" content="Aura's Space"><meta property="og:title" content="【Temperature 1.5 的日常】EP8: 當 Structured Output 遇上評價指標 - 打造自動化的 LLM 裁判員"><meta property="og:description" content=" 本文為個人學習筆記，記錄了學習過程中的一些知識，可參考，但不可認真，學習的過程可能有理解錯誤，資訊不一定正確，畢竟 Temperature 都 1.5 了。
零、 前言 接續上次我們聊到的 自定義中介軟體 (Custom Middleware)，我們掌握了攔截與修改 Agent 執行路徑的能力。但在 AI 應用落地時，最讓開發者頭痛的往往不是「怎麼跑」，而是「跑得好不好」。我們該如何量化一個 RAG（檢索增強生成）系統的表現？
今天我們要將 EP2 提到的結構化輸出 (Structured Output) 與 Azure AI 評價定義 結合。透過定義嚴謹的 Schema 與提示詞，讓 LLM 化身為公正的裁判，為每一次的對話進行精確打分。
一、 評價的三大支柱：Azure AI 評分定義 在 RAG 場景中，要衡量一個系統的品質，通常會參考以下三項關鍵指標。這些定義在 Azure AI 的評分框架中非常完整，我們可以將其直接注入到 Prompt 中：
1. Groundedness (誠實度/接地性) LINK 評估 RESPONSE 是否完全基於提供的 CONTEXT。
核心準則：Context 是唯一的真理。 評分級別： 1 分：完全無關。 2 分：嘗試回應但包含錯誤資訊。 3 分：正確但語意模糊（太過通泛）。 4 分：大部分正確，僅有輕微錯誤。 5 分：完整且精確（包含所有相關細節）。 2. Relevance (相關性) LINK 評估 RESPONSE 是否直接解決了使用者的 QUERY。
核心準則：是否回答了問題？有無洞察力？ 評分級別： 1 分：離題、毫無關聯。 2 分：部分相關但未回答核心問題。 3 分：部分相關但資訊不足。 4 分：高度相關但缺乏深度。 5 分：全面且具備延伸見解。 3. Retrieval (檢索品質) LINK 評估被檢索出的 CONTEXT 塊是否真的對回答問題有幫助。"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-11-01T00:00:00+00:00"><meta property="article:modified_time" content="2025-11-01T00:00:00+00:00"><meta property="article:tag" content="LangChain"><meta property="article:tag" content="Python"><meta property="article:tag" content="Azure AI"><meta name=twitter:card content="summary"><meta name=twitter:title content="【Temperature 1.5 的日常】EP8: 當 Structured Output 遇上評價指標 - 打造自動化的 LLM 裁判員"><meta name=twitter:description content="

本文為個人學習筆記，記錄了學習過程中的一些知識，可參考，但不可認真，學習的過程可能有理解錯誤，資訊不一定正確，畢竟 Temperature 都 1.5 了。


零、 前言
接續上次我們聊到的 自定義中介軟體 (Custom Middleware)，我們掌握了攔截與修改 Agent 執行路徑的能力。但在 AI 應用落地時，最讓開發者頭痛的往往不是「怎麼跑」，而是「跑得好不好」。我們該如何量化一個 RAG（檢索增強生成）系統的表現？
今天我們要將 EP2 提到的結構化輸出 (Structured Output) 與 Azure AI 評價定義 結合。透過定義嚴謹的 Schema 與提示詞，讓 LLM 化身為公正的裁判，為每一次的對話進行精確打分。

一、 評價的三大支柱：Azure AI 評分定義
在 RAG 場景中，要衡量一個系統的品質，通常會參考以下三項關鍵指標。這些定義在 Azure AI 的評分框架中非常完整，我們可以將其直接注入到 Prompt 中：
1. Groundedness (誠實度/接地性) LINK
評估 RESPONSE 是否完全基於提供的 CONTEXT。

核心準則：Context 是唯一的真理。
評分級別：
1 分：完全無關。
2 分：嘗試回應但包含錯誤資訊。
3 分：正確但語意模糊（太過通泛）。
4 分：大部分正確，僅有輕微錯誤。
5 分：完整且精確（包含所有相關細節）。

2. Relevance (相關性) LINK
評估 RESPONSE 是否直接解決了使用者的 QUERY。

核心準則：是否回答了問題？有無洞察力？
評分級別：
1 分：離題、毫無關聯。
2 分：部分相關但未回答核心問題。
3 分：部分相關但資訊不足。
4 分：高度相關但缺乏深度。
5 分：全面且具備延伸見解。

3. Retrieval (檢索品質) LINK
評估被檢索出的 CONTEXT 塊是否真的對回答問題有幫助。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://aura.codex.tw/posts/"},{"@type":"ListItem","position":2,"name":"【Temperature 1.5 的日常】EP8: 當 Structured Output 遇上評價指標 - 打造自動化的 LLM 裁判員","item":"https://aura.codex.tw/posts/temp1.5/ep8/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"【Temperature 1.5 的日常】EP8: 當 Structured Output 遇上評價指標 - 打造自動化的 LLM 裁判員","name":"【Temperature 1.5 的日常】EP8: 當 Structured Output 遇上評價指標 - 打造自動化的 LLM 裁判員","description":" 本文為個人學習筆記，記錄了學習過程中的一些知識，可參考，但不可認真，學習的過程可能有理解錯誤，資訊不一定正確，畢竟 Temperature 都 1.5 了。\n零、 前言 接續上次我們聊到的 自定義中介軟體 (Custom Middleware)，我們掌握了攔截與修改 Agent 執行路徑的能力。但在 AI 應用落地時，最讓開發者頭痛的往往不是「怎麼跑」，而是「跑得好不好」。我們該如何量化一個 RAG（檢索增強生成）系統的表現？\n今天我們要將 EP2 提到的結構化輸出 (Structured Output) 與 Azure AI 評價定義 結合。透過定義嚴謹的 Schema 與提示詞，讓 LLM 化身為公正的裁判，為每一次的對話進行精確打分。\n一、 評價的三大支柱：Azure AI 評分定義 在 RAG 場景中，要衡量一個系統的品質，通常會參考以下三項關鍵指標。這些定義在 Azure AI 的評分框架中非常完整，我們可以將其直接注入到 Prompt 中：\n1. Groundedness (誠實度/接地性) LINK 評估 RESPONSE 是否完全基於提供的 CONTEXT。\n核心準則：Context 是唯一的真理。 評分級別： 1 分：完全無關。 2 分：嘗試回應但包含錯誤資訊。 3 分：正確但語意模糊（太過通泛）。 4 分：大部分正確，僅有輕微錯誤。 5 分：完整且精確（包含所有相關細節）。 2. Relevance (相關性) LINK 評估 RESPONSE 是否直接解決了使用者的 QUERY。\n核心準則：是否回答了問題？有無洞察力？ 評分級別： 1 分：離題、毫無關聯。 2 分：部分相關但未回答核心問題。 3 分：部分相關但資訊不足。 4 分：高度相關但缺乏深度。 5 分：全面且具備延伸見解。 3. Retrieval (檢索品質) LINK 評估被檢索出的 CONTEXT 塊是否真的對回答問題有幫助。\n","keywords":["LangChain","Python","Azure AI"],"articleBody":" 本文為個人學習筆記，記錄了學習過程中的一些知識，可參考，但不可認真，學習的過程可能有理解錯誤，資訊不一定正確，畢竟 Temperature 都 1.5 了。\n零、 前言 接續上次我們聊到的 自定義中介軟體 (Custom Middleware)，我們掌握了攔截與修改 Agent 執行路徑的能力。但在 AI 應用落地時，最讓開發者頭痛的往往不是「怎麼跑」，而是「跑得好不好」。我們該如何量化一個 RAG（檢索增強生成）系統的表現？\n今天我們要將 EP2 提到的結構化輸出 (Structured Output) 與 Azure AI 評價定義 結合。透過定義嚴謹的 Schema 與提示詞，讓 LLM 化身為公正的裁判，為每一次的對話進行精確打分。\n一、 評價的三大支柱：Azure AI 評分定義 在 RAG 場景中，要衡量一個系統的品質，通常會參考以下三項關鍵指標。這些定義在 Azure AI 的評分框架中非常完整，我們可以將其直接注入到 Prompt 中：\n1. Groundedness (誠實度/接地性) LINK 評估 RESPONSE 是否完全基於提供的 CONTEXT。\n核心準則：Context 是唯一的真理。 評分級別： 1 分：完全無關。 2 分：嘗試回應但包含錯誤資訊。 3 分：正確但語意模糊（太過通泛）。 4 分：大部分正確，僅有輕微錯誤。 5 分：完整且精確（包含所有相關細節）。 2. Relevance (相關性) LINK 評估 RESPONSE 是否直接解決了使用者的 QUERY。\n核心準則：是否回答了問題？有無洞察力？ 評分級別： 1 分：離題、毫無關聯。 2 分：部分相關但未回答核心問題。 3 分：部分相關但資訊不足。 4 分：高度相關但缺乏深度。 5 分：全面且具備延伸見解。 3. Retrieval (檢索品質) LINK 評估被檢索出的 CONTEXT 塊是否真的對回答問題有幫助。\n核心準則：最相關的資訊是否排在最前面？ 評分級別： 1 分：檢索內容與 Query 無關。 2 分：檢索內容部分相關但排序不佳。 3 分：相關內容被排在列表底部。 4 分：相關內容排名靠前但不夠精確。 5 分：高度相關且排名精確。 二、 技術核心：利用 Literal 限制分數範圍 為了讓程式碼能穩定處理這些評分，我們不能只讓 LLM 隨意寫一段文字。在 LangChain 中，結合 Pydantic 的 Literal 是最優雅的解法。\n透過 Literal[1, 2, 3, 4, 5]，我們在 Schema 層級就限制了模型的輸出範圍。這確保了模型不會因為「幻覺」而給出 0 分或 6 分，也不會給出「4.5 分」這種無法對接到後端邏輯的模糊值。\n1 2 3 4 5 6 7 from typing import Literal from pydantic import BaseModel, Field class EvaluationResult(BaseModel): thought_chain: str = Field(description=\"逐步分析的推理過程\") explanation: str = Field(description=\"評分理由的簡短說明\") score: Literal[1, 2, 3, 4, 5] = Field(description=\"1到5的整數評分\") 三、 實作流程：從提示詞到物件化 當我們將上述定義整合進 response_format 時，LangChain 會根據模型能力選擇 ProviderStrategy（如 GPT-4o 的原生 JSON mode）或 ToolCallingStrategy。\n1. 注入評價模板 在 System Prompt 中，我們將 Azure AI 的評分標準逐條寫入。例如，針對 Groundedness，我們明確告知模型：\n「如果 RESPONSE 嘗試回應但包含 Context 中未提及的錯誤資訊，應給予 2 分。」\n2. 強制執行 Schema 透過 with_structured_output(EvaluationResult)，我們可以獲得一個乾淨的物件：\n1 2 3 4 5 6 # 呼叫範例 evaluator = model.with_structured_output(EvaluationResult) result = evaluator.invoke(f\"CONTEXT: {context}\\nRESPONSE: {response}\") print(f\"得分：{result.score}\") print(f\"推理：{result.thought_chain}\") 四、 評價任務的層次結構 我們可以針對不同的指標建立不同的「評價專員」：\n專員類型 輸入資料 任務目標 誠實度裁判 Context, Response 抓出模型是否有「一本正經胡說八道」的幻覺。 相關性裁判 Query, Response 判斷模型是否在「顧左右而言他」。 檢索裁判 Query, Context 確保 RAG 的第一步（檢索）沒有撈到垃圾。 五、 為什麼這比「通靈」好？ 在傳統的做法中，我們可能會寫 if \"Score: 5\" in response。但這種做法在遇到多輪對話或模型語氣變化時會失效。\n透過 Structured Output：\n內建校閱：如果模型給出了 1-5 以外的分數，Pydantic 會拋出 ValidationError，這時 Agent 會自動捕捉並回傳錯誤訊息給 LLM，要求它修正。這就是 EP2 提到的「自我修復」機制。 自動化數據化：我們可以將這些 score 直接存入資料庫，繪製成 RAG 系統的品質監控儀表板。 消除模糊：強制模型提供 thought_chain，讓我們能回溯為什麼它會給出這個分數。 六、 實踐技巧：ThoughtChain 的重要性 在評價任務中，我們強烈建議在 Schema 中加入 thought_chain（思維鏈）。 正如 Azure AI 的 Prompt 要求：「Let’s think step by step」。 當模型被要求先寫下分析過程，再給出分數時，其準確度會顯著提升。這是因為模型在生成分數之前，已經在 Token 空間中對 Context 與 Response 進行了比對，這大幅減少了隨機性。\n七、 結語 「無法量化的東西，就無法優化。」\n透過 LangChain 的 Structured Output 結合 Azure AI 的嚴謹定義，我們不再是憑感覺在開發 AI。我們利用 Literal 鎖定了評分邏輯，利用 Pydantic 穩定了輸出結構，將不確定的自然語言評價轉化為確定的軟體數據。\n下次當你覺得你的 Agent 表現不穩時，不妨先幫它請三位「裁判」，看看它在誠實度、相關性與檢索品質上，到底在哪一環掉鏈子了。\n","wordCount":"308","inLanguage":"en","datePublished":"2025-11-01T00:00:00Z","dateModified":"2025-11-01T00:00:00Z","author":{"@type":"Person","name":"Aura"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://aura.codex.tw/posts/temp1.5/ep8/"},"publisher":{"@type":"Organization","name":"Aura's Space","logo":{"@type":"ImageObject","url":"https://aura.codex.tw/images/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://aura.codex.tw/ accesskey=h title="Aura's Space (Alt + H)"><img src=https://aura.codex.tw/images/favicon.ico alt aria-label=logo height=35>Aura's Space</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://aura.codex.tw/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://aura.codex.tw/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://aura.codex.tw/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://aura.codex.tw/about/ title=About><span>About</span></a></li><li><a href=https://aura.codex.tw/search/ title="🔍 (Alt + /)" accesskey=/><span>🔍</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://aura.codex.tw/>Home</a>&nbsp;»&nbsp;<a href=https://aura.codex.tw/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">【Temperature 1.5 的日常】EP8: 當 Structured Output 遇上評價指標 - 打造自動化的 LLM 裁判員</h1><div class=post-meta><span title='2025-11-01 00:00:00 +0000 UTC'>November 1, 2025</span>&nbsp;·&nbsp;<span>2 min</span>&nbsp;·&nbsp;<span>Aura</span></div></header><ul class=post-tags><li><a href=https://aura.codex.tw/tags/langchain/>LangChain</a></li><li><a href=https://aura.codex.tw/tags/python/>Python</a></li><li><a href=https://aura.codex.tw/tags/azure-ai/>Azure AI</a></li></ul><div class=post-content><hr><blockquote><p>本文為個人學習筆記，記錄了學習過程中的一些知識，可參考，但不可認真，學習的過程可能有理解錯誤，資訊不一定正確，畢竟 Temperature 都 1.5 了。</p></blockquote><hr><h3 id=零-前言>零、 前言<a hidden class=anchor aria-hidden=true href=#零-前言>#</a></h3><p>接續上次我們聊到的 <strong>自定義中介軟體 (Custom Middleware)</strong>，我們掌握了攔截與修改 Agent 執行路徑的能力。但在 AI 應用落地時，最讓開發者頭痛的往往不是「怎麼跑」，而是「跑得好不好」。我們該如何量化一個 RAG（檢索增強生成）系統的表現？</p><p>今天我們要將 <strong>EP2 提到的結構化輸出 (Structured Output)</strong> 與 <strong>Azure AI 評價定義</strong> 結合。透過定義嚴謹的 Schema 與提示詞，讓 LLM 化身為公正的裁判，為每一次的對話進行精確打分。</p><hr><h3 id=一-評價的三大支柱azure-ai-評分定義>一、 評價的三大支柱：Azure AI 評分定義<a hidden class=anchor aria-hidden=true href=#一-評價的三大支柱azure-ai-評分定義>#</a></h3><p>在 RAG 場景中，要衡量一個系統的品質，通常會參考以下三項關鍵指標。這些定義在 Azure AI 的評分框架中非常完整，我們可以將其直接注入到 Prompt 中：</p><h4 id=1-groundedness-誠實度接地性-link>1. Groundedness (誠實度/接地性) <a href=https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/evaluation/azure-ai-evaluation/azure/ai/evaluation/_evaluators/_groundedness/groundedness_without_query.prompty>LINK</a><a hidden class=anchor aria-hidden=true href=#1-groundedness-誠實度接地性-link>#</a></h4><p>評估 RESPONSE 是否完全基於提供的 CONTEXT。</p><ul><li><strong>核心準則</strong>：Context 是唯一的真理。</li><li><strong>評分級別</strong>：</li><li><strong>1 分</strong>：完全無關。</li><li><strong>2 分</strong>：嘗試回應但包含錯誤資訊。</li><li><strong>3 分</strong>：正確但語意模糊（太過通泛）。</li><li><strong>4 分</strong>：大部分正確，僅有輕微錯誤。</li><li><strong>5 分</strong>：完整且精確（包含所有相關細節）。</li></ul><h4 id=2-relevance-相關性-link>2. Relevance (相關性) <a href=https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/evaluation/azure-ai-evaluation/azure/ai/evaluation/_evaluators/_relevance/relevance.prompty>LINK</a><a hidden class=anchor aria-hidden=true href=#2-relevance-相關性-link>#</a></h4><p>評估 RESPONSE 是否直接解決了使用者的 QUERY。</p><ul><li><strong>核心準則</strong>：是否回答了問題？有無洞察力？</li><li><strong>評分級別</strong>：</li><li><strong>1 分</strong>：離題、毫無關聯。</li><li><strong>2 分</strong>：部分相關但未回答核心問題。</li><li><strong>3 分</strong>：部分相關但資訊不足。</li><li><strong>4 分</strong>：高度相關但缺乏深度。</li><li><strong>5 分</strong>：全面且具備延伸見解。</li></ul><h4 id=3-retrieval-檢索品質-link>3. Retrieval (檢索品質) <a href=https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/evaluation/azure-ai-evaluation/azure/ai/evaluation/_evaluators/_retrieval/retrieval.prompty>LINK</a><a hidden class=anchor aria-hidden=true href=#3-retrieval-檢索品質-link>#</a></h4><p>評估被檢索出的 CONTEXT 塊是否真的對回答問題有幫助。</p><ul><li><strong>核心準則</strong>：最相關的資訊是否排在最前面？</li><li><strong>評分級別</strong>：</li><li><strong>1 分</strong>：檢索內容與 Query 無關。</li><li><strong>2 分</strong>：檢索內容部分相關但排序不佳。</li><li><strong>3 分</strong>：相關內容被排在列表底部。</li><li><strong>4 分</strong>：相關內容排名靠前但不夠精確。</li><li><strong>5 分</strong>：高度相關且排名精確。</li></ul><hr><h3 id=二-技術核心利用-literal-限制分數範圍>二、 技術核心：利用 Literal 限制分數範圍<a hidden class=anchor aria-hidden=true href=#二-技術核心利用-literal-限制分數範圍>#</a></h3><p>為了讓程式碼能穩定處理這些評分，我們不能只讓 LLM 隨意寫一段文字。在 LangChain 中，結合 Pydantic 的 <code>Literal</code> 是最優雅的解法。</p><p>透過 <code>Literal[1, 2, 3, 4, 5]</code>，我們在 Schema 層級就限制了模型的輸出範圍。這確保了模型不會因為「幻覺」而給出 0 分或 6 分，也不會給出「4.5 分」這種無法對接到後端邏輯的模糊值。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>typing</span> <span class=kn>import</span> <span class=n>Literal</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>pydantic</span> <span class=kn>import</span> <span class=n>BaseModel</span><span class=p>,</span> <span class=n>Field</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>EvaluationResult</span><span class=p>(</span><span class=n>BaseModel</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>thought_chain</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=n>Field</span><span class=p>(</span><span class=n>description</span><span class=o>=</span><span class=s2>&#34;逐步分析的推理過程&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>explanation</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=n>Field</span><span class=p>(</span><span class=n>description</span><span class=o>=</span><span class=s2>&#34;評分理由的簡短說明&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>score</span><span class=p>:</span> <span class=n>Literal</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>5</span><span class=p>]</span> <span class=o>=</span> <span class=n>Field</span><span class=p>(</span><span class=n>description</span><span class=o>=</span><span class=s2>&#34;1到5的整數評分&#34;</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><hr><h3 id=三-實作流程從提示詞到物件化>三、 實作流程：從提示詞到物件化<a hidden class=anchor aria-hidden=true href=#三-實作流程從提示詞到物件化>#</a></h3><p>當我們將上述定義整合進 <code>response_format</code> 時，LangChain 會根據模型能力選擇 <strong>ProviderStrategy</strong>（如 GPT-4o 的原生 JSON mode）或 <strong>ToolCallingStrategy</strong>。</p><h4 id=1-注入評價模板>1. 注入評價模板<a hidden class=anchor aria-hidden=true href=#1-注入評價模板>#</a></h4><p>在 System Prompt 中，我們將 Azure AI 的評分標準逐條寫入。例如，針對 <strong>Groundedness</strong>，我們明確告知模型：</p><blockquote><p>「如果 RESPONSE 嘗試回應但包含 Context 中未提及的錯誤資訊，應給予 2 分。」</p></blockquote><h4 id=2-強制執行-schema>2. 強制執行 Schema<a hidden class=anchor aria-hidden=true href=#2-強制執行-schema>#</a></h4><p>透過 <code>with_structured_output(EvaluationResult)</code>，我們可以獲得一個乾淨的物件：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 呼叫範例</span>
</span></span><span class=line><span class=cl><span class=n>evaluator</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>with_structured_output</span><span class=p>(</span><span class=n>EvaluationResult</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>result</span> <span class=o>=</span> <span class=n>evaluator</span><span class=o>.</span><span class=n>invoke</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;CONTEXT: </span><span class=si>{</span><span class=n>context</span><span class=si>}</span><span class=se>\n</span><span class=s2>RESPONSE: </span><span class=si>{</span><span class=n>response</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;得分：</span><span class=si>{</span><span class=n>result</span><span class=o>.</span><span class=n>score</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;推理：</span><span class=si>{</span><span class=n>result</span><span class=o>.</span><span class=n>thought_chain</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><hr><h3 id=四-評價任務的層次結構>四、 評價任務的層次結構<a hidden class=anchor aria-hidden=true href=#四-評價任務的層次結構>#</a></h3><p>我們可以針對不同的指標建立不同的「評價專員」：</p><table><thead><tr><th>專員類型</th><th>輸入資料</th><th>任務目標</th></tr></thead><tbody><tr><td><strong>誠實度裁判</strong></td><td>Context, Response</td><td>抓出模型是否有「一本正經胡說八道」的幻覺。</td></tr><tr><td><strong>相關性裁判</strong></td><td>Query, Response</td><td>判斷模型是否在「顧左右而言他」。</td></tr><tr><td><strong>檢索裁判</strong></td><td>Query, Context</td><td>確保 RAG 的第一步（檢索）沒有撈到垃圾。</td></tr></tbody></table><hr><h3 id=五-為什麼這比通靈好>五、 為什麼這比「通靈」好？<a hidden class=anchor aria-hidden=true href=#五-為什麼這比通靈好>#</a></h3><p>在傳統的做法中，我們可能會寫 <code>if "Score: 5" in response</code>。但這種做法在遇到多輪對話或模型語氣變化時會失效。</p><p>透過 Structured Output：</p><ol><li><strong>內建校閱</strong>：如果模型給出了 1-5 以外的分數，Pydantic 會拋出 <code>ValidationError</code>，這時 Agent 會自動捕捉並回傳錯誤訊息給 LLM，要求它修正。這就是 <strong>EP2 提到的「自我修復」機制</strong>。</li><li><strong>自動化數據化</strong>：我們可以將這些 score 直接存入資料庫，繪製成 RAG 系統的品質監控儀表板。</li><li><strong>消除模糊</strong>：強制模型提供 <code>thought_chain</code>，讓我們能回溯為什麼它會給出這個分數。</li></ol><hr><h3 id=六-實踐技巧thoughtchain-的重要性>六、 實踐技巧：ThoughtChain 的重要性<a hidden class=anchor aria-hidden=true href=#六-實踐技巧thoughtchain-的重要性>#</a></h3><p>在評價任務中，我們強烈建議在 Schema 中加入 <code>thought_chain</code>（思維鏈）。
正如 Azure AI 的 Prompt 要求：「Let&rsquo;s think step by step」。
當模型被要求先寫下分析過程，再給出分數時，其準確度會顯著提升。這是因為模型在生成分數之前，已經在 Token 空間中對 Context 與 Response 進行了比對，這大幅減少了隨機性。</p><hr><h3 id=七-結語>七、 結語<a hidden class=anchor aria-hidden=true href=#七-結語>#</a></h3><p><strong>「無法量化的東西，就無法優化。」</strong></p><p>透過 LangChain 的 Structured Output 結合 Azure AI 的嚴謹定義，我們不再是憑感覺在開發 AI。我們利用 <code>Literal</code> 鎖定了評分邏輯，利用 Pydantic 穩定了輸出結構，將不確定的自然語言評價轉化為確定的軟體數據。</p><p>下次當你覺得你的 Agent 表現不穩時，不妨先幫它請三位「裁判」，看看它在誠實度、相關性與檢索品質上，到底在哪一環掉鏈子了。</p></div><footer class=post-footer></footer></article></main><footer class=footer><span>&copy; 2022~2026 <a href=https://aura.codex.tw/>Aura's Space</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>