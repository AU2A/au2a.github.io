[{"title":"HakkaASR Website","url":"/2023/06/28/HakkaASR-Website/","content":"連結DockerGithub\nDocker 執行使用Docker時，可以使用以下指令：\ndocker run -d -e psw=a_string -e domain=a_domain \\        -p 443:443 -p 5002:5002 au2a/hakka-website\npsw為密碼，需要密碼才能執行，如需要請向我詢問。domain為網址，設定網頁使用的網址，一開始可以使用hakka.corelab.dev會導向到127.0.0.1-p 443:443網頁使用的是https通道-p 5002:5002網頁解碼使用的通道\n專案結構files ┗┳━ decode  ┃   ┗━ espnet解碼後的文本  ┣━ initFiles  ┃   ┗━ 密碼的md5較驗檔、需要初始化的檔案  ┣━ keys  ┃   ┗━ 安全憑證的密碼，和cert.pem, chain.pem, privkey.pem三個憑證檔案  ┣━ openai  ┃   ┣━ decode  ┃   ┃   ┗━openai whisper解碼後的文本  ┃   ┣━ model  ┃   ┃   ┗━openai whisper使用的model  ┃   ┣━ upload  ┃   ┃   ┗━上傳openai whisper解碼的音檔存放區  ┃   ┣━ delete.py 定期刪除存放太久的音檔語文本  ┃   ┣━ download.py 下載youtube檔案轉wav檔  ┃   ┗━ openai_whisper.py Openai whisper解碼主程式  ┣━ upload  ┃   ┗━ 上傳espnet解碼的音檔存放區  ┣━ views  ┃   ┗━ 網頁  ┣━ website  ┃   ┣━ demo  ┃   ┃   ┗━範例音檔與文本  ┃   ┣━ files  ┃   ┃   ┗━網頁主題、網頁JS程式碼  ┃   ┗━ server.js 伺服器主程式  ┣━ aidecodeList.txt 待whisper解碼清單  ┣━ decodeList.txt 待espnet解碼清單  ┣━ domainName 網址  ┣━ init.py 初始化  ┣━ run.sh docker用啟動執行檔  ┗━ test.sh 測試用啟動執行檔\n\n執行全部執行如果要在本地端執行，請執行test.sh\n# 將hakka.corelab.dev文字放入domainNameecho &quot;hakka.corelab.dev&quot; &gt; domainName# 初始化所有檔案python3 init.py# 啟動三個解碼執行緒、自動刪除舊檔案與網頁伺服器python3 openai/openai_whisper.py &amp; python3 openai/openai_whisper.py &amp; python3 openai/openai_whisper.py &amp; python3 openai/delete.py &amp; node website/server.js\n分開執行請在專案目錄下執行檔案，不然部分指令相對路徑會錯誤\n網頁執行執行網頁前，需先安裝網頁需要的套件請先到website/底下安裝\n~/Hakka_Website/website$ npm install\n在回到專案目錄執行網頁\n~/Hakka_Website$ node website/server.js\nwhisper解碼執行~/Hakka_Website$ python3 openai/openai_whisper.py\n\n安全憑證因為網頁有使用的錄音功能，所以網頁需要走https協定，因此需要申請安全憑證。我是使用LetsEncrypt的安全憑證，每90天需要重新申請一次。參考網站請將申請後的安全憑證檔案放到keys/底下即可運作網頁。檔案有三：cert.pem, chain.pem, privkey.pem\n"},{"title":"Openai Whisper Fine-Tuning - Hakka","url":"/2023/07/04/Openai-Whisper-Fine-Tuning-Hakka/","content":"專案結構whisper_hakka ┗┳━ audio  ┃   ┣━ test  ┃   ┃   ┗━ test語料存放區  ┃   ┣━ train  ┃   ┃   ┗━ train語料存放區  ┃   ┗━ metadata.csv 檔案路徑與文本內容  ┣━ model  ┃   ┗━ 模型存放區  ┣━ original_audio  ┃   ┗━ 原始音檔，有0.9, 1.0, 1.1倍速三組  ┣━ text  ┃   ┣━ nameAndText.txt 文本  ┃   ┗━ 文本相關檔案  ┣━ fine_tune.ipynb jupyter訓練腳本(建議用這個)  ┗━ run.py 訓練腳本\n\n程式碼說明建立模型名稱請輸入檔案名稱\nmodel_name=&#x27;file_name&#x27;\n登入hugging face將訓練完成的模型上傳到存放在huggingface，可以減少本地端空間占用。Token請自行去huggingface申請\nfrom huggingface_hub.hf_api import HfFolderHfFolder.save_token(&quot;Token from huggingface&quot;)\nprint(common_voice[‘train’][0])\n載入音檔資料會從data_dir底下拉語料進行使用\nfrom datasets import load_datasetcommon_voice = load_dataset(&quot;./&quot;, data_dir=&quot;audio&quot;,use_auth_token=True)# 可以使用以下程式碼查看dataset結構print(common_voice)\n載入Openai建立好的模型from transformers import WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessorfeature_extractor = WhisperFeatureExtractor.from_pretrained(&quot;openai/whisper-base&quot;)tokenizer = WhisperTokenizer.from_pretrained(&quot;openai/whisper-base&quot;, language=&quot;zh&quot;, task=&quot;transcribe&quot;)processor = WhisperProcessor.from_pretrained(&quot;openai/whisper-base&quot;, language=&quot;zh&quot;, task=&quot;transcribe&quot;)\n語料轉換取樣率from datasets import Audiocommon_voice = common_voice.cast_column(&quot;audio&quot;, Audio(sampling_rate=16000))def prepare_dataset(batch):    audio = batch[&quot;audio&quot;]    batch[&quot;input_features&quot;] = feature_extractor(audio[&quot;array&quot;], sampling_rate=audio[&quot;sampling_rate&quot;]).input_features[0]    batch[&quot;labels&quot;] = tokenizer(batch[&quot;sentence&quot;]).input_ids    return batchcommon_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[&quot;train&quot;], num_proc=2)\ndata_collatorimport torchfrom dataclasses import dataclassfrom typing import Any, Dict, List, Union@dataclassclass DataCollatorSpeechSeq2SeqWithPadding:    processor: Any    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -&gt; Dict[str, torch.Tensor]:        # split inputs and labels since they have to be of different lengths and need different padding methods        # first treat the audio inputs by simply returning torch tensors        input_features = [&#123;&quot;input_features&quot;: feature[&quot;input_features&quot;]&#125; for feature in features]        batch = self.processor.feature_extractor.pad(input_features, return_tensors=&quot;pt&quot;)        # get the tokenized label sequences        label_features = [&#123;&quot;input_ids&quot;: feature[&quot;labels&quot;]&#125; for feature in features]        # pad the labels to max length        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=&quot;pt&quot;)        # replace padding with -100 to ignore loss correctly        labels = labels_batch[&quot;input_ids&quot;].masked_fill(labels_batch.attention_mask.ne(1), -100)        # if bos token is appended in previous tokenization step,        # cut bos token here as it&#x27;s append later anyways        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():            labels = labels[:, 1:]        batch[&quot;labels&quot;] = labels        return batchdata_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\ncompute_metricsimport evaluatemetric = evaluate.load(&quot;wer&quot;)def compute_metrics(pred):    pred_ids = pred.predictions    label_ids = pred.label_ids    # replace -100 with the pad_token_id    label_ids[label_ids == -100] = tokenizer.pad_token_id    # we do not want to group tokens when computing the metrics    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)    wer = 100 * metric.compute(predictions=pred_str, references=label_str)    return &#123;&quot;wer&quot;: wer&#125;\nmodelfrom transformers import WhisperForConditionalGenerationmodel = WhisperForConditionalGeneration.from_pretrained(&quot;openai/whisper-base&quot;)model.config.forced_decoder_ids = Nonemodel.config.suppress_tokens = []\ntraining_argsfrom transformers import Seq2SeqTrainingArgumentstraining_args = Seq2SeqTrainingArguments(    output_dir=&quot;./model_name&quot;, # 模型名稱，你需要更改    per_device_train_batch_size=16, # 批次大小，你可能會需要調整    gradient_accumulation_steps=1,    learning_rate=1e-5, # 學習率，你可能會需要調整    warmup_steps=500,    max_steps=4000, # 訓練次數，你可能會需要調整    gradient_checkpointing=True,    fp16=True,    evaluation_strategy=&quot;steps&quot;,    per_device_eval_batch_size=8,    predict_with_generate=True,    generation_max_length=225,    save_steps=1000,    eval_steps=1000,    logging_steps=25,    report_to=[&quot;tensorboard&quot;],    load_best_model_at_end=True,    metric_for_best_model=&quot;wer&quot;,    greater_is_better=False,    push_to_hub=True,)\ntrainerfrom transformers import Seq2SeqTrainertrainer = Seq2SeqTrainer(    args=training_args,    model=model,    train_dataset=common_voice[&quot;train&quot;],    eval_dataset=common_voice[&quot;test&quot;],    data_collator=data_collator,    compute_metrics=compute_metrics,    tokenizer=processor.feature_extractor,)processor.save_pretrained(training_args.output_dir)\n開始訓練trainer.train()\n從本地上傳模型到HuggingFacekwargs = &#123;    &quot;dataset_tags&quot;: &quot;-&quot;,    &quot;dataset&quot;: &quot;some hakka audio&quot;,  # 輸入資料及名稱    &quot;dataset_args&quot;: &quot;config: zh, split: test&quot;,    &quot;language&quot;: &quot;zh&quot;,     &quot;model_name&quot;: &quot;a name&quot;,  # 輸入模型名稱    &quot;finetuned_from&quot;: &quot;openai/whisper-base&quot;, # 基礎模型    &quot;tasks&quot;: &quot;automatic-speech-recognition&quot;,    &quot;tags&quot;: &quot;whisper&quot;,&#125;trainer.push_to_hub(**kwargs)\n從HuggingFace下載模型from multiple_datasets.hub_default_utils import convert_hf_whisper# 你需要更改要下載model的位置與存放位置# Ex.model_name_or_path = &#x27;model_name_on_hugging_face&#x27;whisper_checkpoint_path = &#x27;save_model_path&#x27;convert_hf_whisper(model_name_or_path, whisper_checkpoint_path)\n\nreferencehttps://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/fine_tune_whisper.ipynb#scrollTo=810ced54-7187-4a06-b2fe-ba6dcca94dc3https://colab.research.google.com/drive/1RkboArXsuXIEDTE5OHfJe-0Gn7v3gXI1?usp=sharing#scrollTo=-hxbi4vVPpoyhttps://wandb.ai/parambharat/whisper_finetuning/reports/Fine-tuning-Whisper-ASR-models---VmlldzozMTEzNDE5https://huggingface.co/jlondonobo/whisper-medium-pthttps://github.com/bayartsogt-ya/whisper-multiple-hf-datasetshttps://github.com/luigisaetta/whisper-app/blob/main/match_layers.ipynbhttps://www.mlq.ai/openai-whisper-gpt-3-fine-tuning-youtube-video/https://stackoverflow.com/questions/71561761/how-to-load-a-fine-tuned-pytorch-huggingface-bert-model-from-a-checkpoint-filehttps://colab.research.google.com/drive/1P4ClLkPmfsaKn2tBbRp0nVjGMRKR-EWzhttps://huggingface.co/spaces/openai/whisper/discussions/6https://huggingface.co/blog/fine-tune-whisperhttps://github.com/openai/whisper/discussions/98\n"}]