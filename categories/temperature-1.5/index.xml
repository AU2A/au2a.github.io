<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Temperature 1.5 on Aura&#39;s Space</title>
    <link>https://aura.codex.tw/categories/temperature-1.5/</link>
    <description>Recent content in Temperature 1.5 on Aura&#39;s Space</description>
    <generator>Hugo -- 0.153.2</generator>
    <language>en</language>
    <lastBuildDate>Mon, 10 Nov 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://aura.codex.tw/categories/temperature-1.5/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>【Temperature 1.5 的日常】EP11: 穿越時空的記憶 - LangChain Long-term Memory 與 Store 實戰</title>
      <link>https://aura.codex.tw/posts/temp1.5/ep11/</link>
      <pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate>
      <guid>https://aura.codex.tw/posts/temp1.5/ep11/</guid>
      <description>&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;本文為個人學習筆記，記錄了學習過程中的一些知識，可參考，但不可認真，學習的過程可能有理解錯誤，資訊不一定正確，畢竟 Temperature 都 1.5 了。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;零-前言&#34;&gt;零、 前言&lt;/h3&gt;
&lt;p&gt;接續上次我們聊到的 &lt;strong&gt;Short-term Memory (短期記憶)&lt;/strong&gt;，我們透過 Checkpointer 讓 Agent 能夠在同一個 Thread (對話串) 中記住上下文，解決了「金魚腦」的問題。&lt;/p&gt;
&lt;p&gt;但問題來了，如果使用者關掉視窗，明天開啟一個&lt;strong&gt;新的對話串 (New Thread)&lt;/strong&gt; 呢？
在預設情況下，Agent 會把昨天的事忘得一乾二淨。這就像你每天去同一家咖啡廳，店員卻每天都問你：「先生貴姓？喝什麼？」這體驗肯定不好。&lt;/p&gt;
&lt;p&gt;為了讓 Agent 能夠跨越對話串，記住使用者的偏好（例如：只講中文、喜歡簡短回答），我們需要引入 &lt;strong&gt;Long-term Memory (長期記憶)&lt;/strong&gt;。今天就來看看 LangChain 文件中提到的 &lt;strong&gt;LangGraph Store&lt;/strong&gt; 機制。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;一-長期記憶的本質store-與-json&#34;&gt;一、 長期記憶的本質：Store 與 JSON&lt;/h3&gt;
&lt;p&gt;不同於短期記憶是把對話歷史 (Messages) 存起來，LangChain 定義的長期記憶更像是一個資料庫。&lt;/p&gt;
&lt;p&gt;根據文件，LangGraph 使用 &lt;strong&gt;Store&lt;/strong&gt; 來保存這些記憶，格式是 &lt;strong&gt;JSON Documents&lt;/strong&gt;。這讓記憶不僅僅是文字，而是結構化的資料。&lt;/p&gt;
&lt;h4 id=&#34;核心結構namespace-與-key&#34;&gt;核心結構：Namespace 與 Key&lt;/h4&gt;
&lt;p&gt;為了不讓記憶變成一團亂麻，LangGraph 採用了類似檔案系統的結構：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Namespace (命名空間)&lt;/strong&gt;：
這就像是「資料夾」。通常會包含使用者 ID、組織 ID 或其他標籤。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;格式範例：&lt;code&gt;(user_id, application_context)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;這讓我們可以輕鬆區分不同使用者的記憶，甚至支援階層式的組織。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Key (鍵)&lt;/strong&gt;：
這就像是「檔名」。在同一個 Namespace 下，Key 必須是唯一的。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;這種結構的強大之處在於，它支援 &lt;strong&gt;跨 Namespace 搜尋&lt;/strong&gt;。你可以透過 Content Filters (內容過濾) 來查找特定條件的記憶。&lt;/p&gt;</description>
    </item>
    <item>
      <title>【Temperature 1.5 的日常】EP10: 拒絕金魚腦 - 深入解析 LangChain 的 Short-term Memory 與狀態管理</title>
      <link>https://aura.codex.tw/posts/temp1.5/ep10/</link>
      <pubDate>Fri, 07 Nov 2025 00:00:00 +0000</pubDate>
      <guid>https://aura.codex.tw/posts/temp1.5/ep10/</guid>
      <description>&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;本文為個人學習筆記，記錄了學習過程中的一些知識，可參考，但不可認真，學習的過程可能有理解錯誤，資訊不一定正確，畢竟 Temperature 都 1.5 了。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;零-前言&#34;&gt;零、 前言&lt;/h3&gt;
&lt;p&gt;接續上次我們聊到的 &lt;strong&gt;MCP (Model Context Protocol)&lt;/strong&gt;，我們讓 Agent 能夠透過通用標準連接各式各樣的工具與資源。但如果你的 Agent 連接了全世界，卻轉頭就忘記你是誰，那也是白搭。&lt;/p&gt;
&lt;p&gt;在複雜的對話任務中，記憶 (Memory) 是核心。今天我們要來探討 LangChain 文件中關於 &lt;strong&gt;Short-term memory (短期記憶)&lt;/strong&gt; 的實作機制。如何在有限的 Context Window 內，讓 Agent 既能記得之前的互動，又不會因為資訊過載而「分心」或變慢？讓我們看下去。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;一-什麼是短期記憶與-thread&#34;&gt;一、 什麼是短期記憶與 Thread？&lt;/h3&gt;
&lt;p&gt;在 LangChain 的定義裡，短期記憶主要指的是 &lt;strong&gt;Thread-level persistence&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;這概念很像 Email 的群組信件或是 Slack 的討論串。一個 &lt;strong&gt;Thread&lt;/strong&gt; 組織了單一會話中的所有互動。這對於 Agent 至關重要，因為它需要：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;記住之前的互動（Context）。&lt;/li&gt;
&lt;li&gt;從回饋中學習。&lt;/li&gt;
&lt;li&gt;適應使用者的偏好。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;然而，挑戰在於 LLM 的 &lt;strong&gt;Context Window (上下文視窗)&lt;/strong&gt; 是有限的。即便模型支援超長文本，塞入過多舊資訊會導致模型「分心 (distracted)」，反應變慢且成本變高。因此，我們需要一套機制來管理這些訊息流。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;二-啟用記憶checkpointer-是關鍵&#34;&gt;二、 啟用記憶：Checkpointer 是關鍵&lt;/h3&gt;
&lt;p&gt;要讓 Agent 擁有記憶，你不能只是把它跑起來，你需要在建立 Agent 時指定一個 &lt;strong&gt;&lt;code&gt;checkpointer&lt;/code&gt;&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;LangChain 的 Agent 將記憶視為 &lt;strong&gt;State (狀態)&lt;/strong&gt; 的一部分。&lt;/p&gt;</description>
    </item>
    <item>
      <title>【Temperature 1.5 的日常】EP9: 當 Agent 遇上通用標準 - Model Context Protocol (MCP) 與 LangChain 的整合之道</title>
      <link>https://aura.codex.tw/posts/temp1.5/ep9/</link>
      <pubDate>Wed, 05 Nov 2025 00:00:00 +0000</pubDate>
      <guid>https://aura.codex.tw/posts/temp1.5/ep9/</guid>
      <description>&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;本文為個人學習筆記，記錄了學習過程中的一些知識，可參考，但不可認真，學習的過程可能有理解錯誤，資訊不一定正確，畢竟 Temperature 都 1.5 了。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;零-前言&#34;&gt;零、 前言&lt;/h3&gt;
&lt;p&gt;接續上次我們聊到的 &lt;strong&gt;自動化評價裁判 (Automated Evaluators)&lt;/strong&gt;，我們確保了 Agent 的輸出品質。但在解決了「好不好」的問題後，我們馬上遇到了「通不通」的問題。隨著 Agent 生態系的爆炸，每個工具、每個資料庫都有自己的 API 接口，讓 Agent 與外部世界溝通變得像是在蓋巴別塔。&lt;/p&gt;
&lt;p&gt;今天我們要來聊聊 &lt;strong&gt;Model Context Protocol (MCP)&lt;/strong&gt;，這是一個試圖標準化應用程式如何向 LLM 提供工具與 Context 的開放協議。而在 LangChain 中，透過 &lt;code&gt;langchain-mcp-adapters&lt;/code&gt;，我們能輕鬆地讓 Agent 連接上這個日益龐大的生態系。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;一-什麼是-mcp為什麼我們需要它&#34;&gt;一、 什麼是 MCP？為什麼我們需要它？&lt;/h3&gt;
&lt;p&gt;簡單來說，MCP 就像是 AI 時代的 USB Type-C。它定義了一套標準，讓開發者只需寫一次 Server，就能被所有支援 MCP 的 Client（如 Claude Desktop 或你的 LangChain Agent）讀取。&lt;/p&gt;
&lt;p&gt;在 LangChain 中，我們主要透過 &lt;code&gt;langchain-mcp-adapters&lt;/code&gt; 這個函式庫來實現對接。它最大的賣點是 &lt;strong&gt;&lt;code&gt;MultiServerMCPClient&lt;/code&gt;&lt;/strong&gt;，這個客戶端允許你的 Agent 同時連接多個 MCP 伺服器，無論是本地的還是遠端的。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 安裝適配器&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install langchain-mcp-adapters
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 或者使用 uv&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;uv add langchain-mcp-adapters
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;hr&gt;
&lt;h3 id=&#34;二-連接世界transports-與-multiservermcpclient&#34;&gt;二、 連接世界：Transports 與 MultiServerMCPClient&lt;/h3&gt;
&lt;p&gt;MCP 支援不同的傳輸方式 (Transports)，LangChain 的適配器完美支援了這兩種主流模式：&lt;/p&gt;</description>
    </item>
    <item>
      <title>【Temperature 1.5 的日常】EP8: 當 Structured Output 遇上評價指標 - 打造自動化的 LLM 裁判員</title>
      <link>https://aura.codex.tw/posts/temp1.5/ep8/</link>
      <pubDate>Sat, 01 Nov 2025 00:00:00 +0000</pubDate>
      <guid>https://aura.codex.tw/posts/temp1.5/ep8/</guid>
      <description>&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;本文為個人學習筆記，記錄了學習過程中的一些知識，可參考，但不可認真，學習的過程可能有理解錯誤，資訊不一定正確，畢竟 Temperature 都 1.5 了。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;零-前言&#34;&gt;零、 前言&lt;/h3&gt;
&lt;p&gt;接續上次我們聊到的 &lt;strong&gt;自定義中介軟體 (Custom Middleware)&lt;/strong&gt;，我們掌握了攔截與修改 Agent 執行路徑的能力。但在 AI 應用落地時，最讓開發者頭痛的往往不是「怎麼跑」，而是「跑得好不好」。我們該如何量化一個 RAG（檢索增強生成）系統的表現？&lt;/p&gt;
&lt;p&gt;今天我們要將 &lt;strong&gt;EP2 提到的結構化輸出 (Structured Output)&lt;/strong&gt; 與 &lt;strong&gt;Azure AI 評價定義&lt;/strong&gt; 結合。透過定義嚴謹的 Schema 與提示詞，讓 LLM 化身為公正的裁判，為每一次的對話進行精確打分。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;一-評價的三大支柱azure-ai-評分定義&#34;&gt;一、 評價的三大支柱：Azure AI 評分定義&lt;/h3&gt;
&lt;p&gt;在 RAG 場景中，要衡量一個系統的品質，通常會參考以下三項關鍵指標。這些定義在 Azure AI 的評分框架中非常完整，我們可以將其直接注入到 Prompt 中：&lt;/p&gt;
&lt;h4 id=&#34;1-groundedness-誠實度接地性-link&#34;&gt;1. Groundedness (誠實度/接地性) &lt;a href=&#34;https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/evaluation/azure-ai-evaluation/azure/ai/evaluation/_evaluators/_groundedness/groundedness_without_query.prompty&#34;&gt;LINK&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;評估 RESPONSE 是否完全基於提供的 CONTEXT。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;核心準則&lt;/strong&gt;：Context 是唯一的真理。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;評分級別&lt;/strong&gt;：&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;1 分&lt;/strong&gt;：完全無關。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2 分&lt;/strong&gt;：嘗試回應但包含錯誤資訊。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;3 分&lt;/strong&gt;：正確但語意模糊（太過通泛）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;4 分&lt;/strong&gt;：大部分正確，僅有輕微錯誤。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;5 分&lt;/strong&gt;：完整且精確（包含所有相關細節）。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;2-relevance-相關性-link&#34;&gt;2. Relevance (相關性) &lt;a href=&#34;https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/evaluation/azure-ai-evaluation/azure/ai/evaluation/_evaluators/_relevance/relevance.prompty&#34;&gt;LINK&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;評估 RESPONSE 是否直接解決了使用者的 QUERY。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;核心準則&lt;/strong&gt;：是否回答了問題？有無洞察力？&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;評分級別&lt;/strong&gt;：&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;1 分&lt;/strong&gt;：離題、毫無關聯。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2 分&lt;/strong&gt;：部分相關但未回答核心問題。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;3 分&lt;/strong&gt;：部分相關但資訊不足。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;4 分&lt;/strong&gt;：高度相關但缺乏深度。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;5 分&lt;/strong&gt;：全面且具備延伸見解。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;3-retrieval-檢索品質-link&#34;&gt;3. Retrieval (檢索品質) &lt;a href=&#34;https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/evaluation/azure-ai-evaluation/azure/ai/evaluation/_evaluators/_retrieval/retrieval.prompty&#34;&gt;LINK&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;評估被檢索出的 CONTEXT 塊是否真的對回答問題有幫助。&lt;/p&gt;</description>
    </item>
    <item>
      <title>【Temperature 1.5 的日常】EP7: LangChain - Custom Middleware, 打造量身定制的 Agent 攔截器</title>
      <link>https://aura.codex.tw/posts/temp1.5/ep7/</link>
      <pubDate>Thu, 30 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://aura.codex.tw/posts/temp1.5/ep7/</guid>
      <description>&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;本文為個人學習筆記，記錄了學習過程中的一些知識，可參考，但不可認真，學習的過程可能有理解錯誤，資訊不一定正確，畢竟 Temperature 都 1.5 了。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;零-前言&#34;&gt;零、 前言&lt;/h3&gt;
&lt;p&gt;接續上次我們聊到的 &lt;strong&gt;內建中介軟體 (Built-in Middleware)&lt;/strong&gt;，雖然官方提供的工具箱已經非常強大，但在現實的開發場景中，我們總會遇到一些「非典型」需求。比如：你想在對話達到 50 句時強制結束、你想根據使用者的 VIP 等級切換不同的模型、或者你想在特定工具執行失敗時進行客製化的重試邏輯。&lt;/p&gt;
&lt;p&gt;這時候，我們就需要動手寫 &lt;strong&gt;「自定義中介軟體」(Custom Middleware)&lt;/strong&gt;。它就像是給 Agent 裝上了一層透明的攔截器，讓你能在 Agent 執行的各個關鍵節點（Hooks）插入自己的邏輯，甚至改變 Agent 的執行路徑。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;一-核心概念兩大掛鉤類型&#34;&gt;一、 核心概念：兩大掛鉤類型&lt;/h3&gt;
&lt;p&gt;在 LangChain 的自定義中介軟體中，主要分為兩種風格的「掛鉤」（Hooks）：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;類型&lt;/th&gt;
          &lt;th&gt;說明&lt;/th&gt;
          &lt;th&gt;適用場景&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Node-style hooks&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;在特定的執行點「依序」運行。&lt;/td&gt;
          &lt;td&gt;日誌記錄 (Logging)、資料驗證、狀態更新。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Wrap-style hooks&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;「包裹」在模型或工具呼叫的周圍。&lt;/td&gt;
          &lt;td&gt;快取 (Caching)、自定義重試、短路邏輯。&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h3 id=&#34;二-node-style-hooks精準的時間點控制&#34;&gt;二、 Node-style hooks：精準的時間點控制&lt;/h3&gt;
&lt;p&gt;Node-style hooks 讓你能在 Agent 生命週期的特定時刻介入。&lt;/p&gt;
&lt;h4 id=&#34;1-可用的掛鉤點&#34;&gt;1. 可用的掛鉤點&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;before_agent&lt;/code&gt;: Agent 開始執行前（每次調用僅一次）。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;before_model&lt;/code&gt;: 每次模型呼叫前。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;after_model&lt;/code&gt;: 每次模型回應後。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;after_agent&lt;/code&gt;: Agent 執行完成後（每次調用僅一次）。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;2-實作範例限制訊息數量&#34;&gt;2. 實作範例：限制訊息數量&lt;/h4&gt;
&lt;p&gt;如果你想防止對話過長導致 Token 爆表，可以在 &lt;code&gt;before_model&lt;/code&gt; 檢查訊息長度，並利用 &lt;code&gt;jump_to&lt;/code&gt; 提前結束：&lt;/p&gt;</description>
    </item>
    <item>
      <title>【Temperature 1.5 的日常】EP6: LangChain - Built-in Middleware, 打造 AI 的全能工具箱</title>
      <link>https://aura.codex.tw/posts/temp1.5/ep6/</link>
      <pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://aura.codex.tw/posts/temp1.5/ep6/</guid>
      <description>&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;本文為個人學習筆記，記錄了學習過程中的一些知識，可參考，但不可認真，學習的過程可能有理解錯誤，資訊不一定正確，畢竟 Temperature 都 1.5 了。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;零-前言&#34;&gt;零、 前言&lt;/h3&gt;
&lt;p&gt;接續上次我們聊到的 &lt;strong&gt;Human-in-the-Loop (HITL)&lt;/strong&gt;，我們知道為了安全，必須在 AI 動手前加個「緊急煞車」。但開發一個生產等級的 Agent，光有煞車是不夠的，你可能還需要一個自動清理垃圾的清潔員、一個幫你省錢的會計師，甚至是一個能在主線掛掉時自動接手的備援司機。&lt;/p&gt;
&lt;p&gt;在 LangChain 的世界裡，這些功能不必從零開始寫。官方提供了一整套「內建中介軟體」（Built-in Middleware），它們就像是 Agent 的各種外掛模組，插上去就能用。今天我們就來開箱這個萬能工具箱。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;一-什麼是內建中介軟體&#34;&gt;一、 什麼是內建中介軟體？&lt;/h3&gt;
&lt;p&gt;簡單來說，中介軟體（Middleware）是介於「使用者輸入」與「模型回應」之間的邏輯層。LangChain 的內建中介軟體已經針對常見的 Agent 使用場景進行了最佳化，且大部份是 &lt;strong&gt;Provider-agnostic&lt;/strong&gt;（與供應商無關），無論你底層用的是 OpenAI、Anthropic 還是本地的 Llama，通通適用。&lt;/p&gt;
&lt;p&gt;以下是目前官方提供的核心中介軟體清單：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;中介軟體&lt;/th&gt;
          &lt;th&gt;主要功能&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Summarization&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;當 Token 接近上限時，自動摘要對話歷史。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Model/Tool Call Limit&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;限制模型或工具的調用次數，防止成本失控。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Model Fallback&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;主模型失效時，自動切換到備援模型。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;PII Detection&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;偵測並處理個人敏感資訊（如姓名、信用卡號）。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Tool/Model Retry&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;遇到暫時性錯誤時，自動進行指數退避重試。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Context Editing&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;精細管理上下文，例如清除舊的工具輸出。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Shell / File Search&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;提供持久的 Shell 環境或檔案搜尋能力。&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h3 id=&#34;二-token-的管理大師summarization--context-editing&#34;&gt;二、 Token 的管理大師：Summarization &amp;amp; Context Editing&lt;/h3&gt;
&lt;p&gt;AI 最怕的就是「忘東忘西」或是「話太多」導致 Token 爆量。&lt;/p&gt;
&lt;h4 id=&#34;1-summarization-middleware&#34;&gt;1. Summarization Middleware&lt;/h4&gt;
&lt;p&gt;當對話變長時，這個中介軟體會自動觸發。它會保留最近的幾則訊息，並將更早之前的對話壓縮成一段摘要。&lt;/p&gt;</description>
    </item>
    <item>
      <title>【Temperature 1.5 的日常】EP5: LangChain - Human-in-the-Loop, 給 AI 的「緊急煞車」</title>
      <link>https://aura.codex.tw/posts/temp1.5/ep5/</link>
      <pubDate>Sat, 25 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://aura.codex.tw/posts/temp1.5/ep5/</guid>
      <description>&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;本文為個人學習筆記，記錄了學習過程中的一些知識，可參考，但不可認真，學習的過程可能有理解錯誤，資訊不一定正確，畢竟 Temperature 都 1.5 了。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;零-前言&#34;&gt;零、 前言&lt;/h3&gt;
&lt;p&gt;接續上次我們聊到的 &lt;strong&gt;Streaming&lt;/strong&gt;，串流讓 AI 有了「呼吸感」，但當 AI 的「手」（Tools）伸向一些敏感區域時，光有呼吸是不夠的。&lt;/p&gt;
&lt;p&gt;想像一下：如果你讓 AI 幫你管理資料庫，它突然決定執行一條 &lt;code&gt;DELETE FROM records&lt;/code&gt; 刪掉你過去 30 天的資料；或者它寫了一封語氣奇怪的信準備寄給你的大老闆。這時候，你需要的不是看著它「順暢地」把錯事做完，而是一個能讓它停下來、等你點頭的機制。這就是我們今天要聊的 &lt;strong&gt;Human-in-the-Loop (HITL)&lt;/strong&gt;。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;一-什麼是-human-in-the-loop&#34;&gt;一、 什麼是 Human-in-the-Loop？&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Human-in-the-Loop (HITL)&lt;/strong&gt; 是一種中介軟體（Middleware）機制。它在模型「提案」執行某個工具（Tool Call）與「實際執行」之間加了一道關卡。&lt;/p&gt;
&lt;p&gt;當模型提出一些可能有風險的操作（例如：寫入檔案、執行 SQL）時，中介軟體會根據預設的策略（Policy）攔截這個動作，將執行狀態「暫停」並保存起來，等待人類的最終決定。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;二-hitl-的核心三要素&#34;&gt;二、 HITL 的核心三要素&lt;/h3&gt;
&lt;p&gt;要實作這套「人工審核」機制，LangChain 依賴以下三個核心組件：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Interrupt (中斷)&lt;/strong&gt;：當符合攔截條件時，系統發出信號停止執行。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Persistence (持久層)&lt;/strong&gt;：使用 LangGraph 的檢查點（Checkpointer）來保存當前的圖形狀態（Graph State）。這確保了程序可以在暫停後，即使重啟也能從原點恢復。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Configurable Policy (配置策略)&lt;/strong&gt;：定義哪些工具需要被監督。&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id=&#34;三-三種人類抉擇approve-edit-reject&#34;&gt;三、 三種人類抉擇：Approve, Edit, Reject&lt;/h3&gt;
&lt;p&gt;當 AI 被攔截後，身為「主管」的你有三種處理方式：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;決策類型&lt;/th&gt;
          &lt;th&gt;說明&lt;/th&gt;
          &lt;th&gt;範例&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;✅ &lt;strong&gt;&lt;code&gt;approve&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;原封不動核准執行。&lt;/td&gt;
          &lt;td&gt;郵件草稿沒問題，直接寄出。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;✏️ &lt;strong&gt;&lt;code&gt;edit&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;修改工具參數後再執行。&lt;/td&gt;
          &lt;td&gt;將郵件的收件人從 A 改成 B 再寄出。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;❌ &lt;strong&gt;&lt;code&gt;reject&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;拒絕執行，並給予 AI 反饋。&lt;/td&gt;
          &lt;td&gt;拒絕刪除指令，並告訴 AI：「這太危險了，請改用查詢。」&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;：在使用 &lt;code&gt;edit&lt;/code&gt; 修改參數時，建議進行保守的改動。劇烈的修改可能會導致模型重新評估其策略，進而引發非預期的後續行為。&lt;/p&gt;</description>
    </item>
    <item>
      <title>【Temperature 1.5 的日常】EP4: LangChain - Streaming, 打破沈默的呼吸感</title>
      <link>https://aura.codex.tw/posts/temp1.5/ep4/</link>
      <pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://aura.codex.tw/posts/temp1.5/ep4/</guid>
      <description>&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;本文為個人學習筆記，記錄了學習過程中的一些知識，可參考，但不可認真，學習的過程可能有理解錯誤，資訊不一定正確，畢竟 Temperature 都 1.5 了。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;零-前言&#34;&gt;零、 前言&lt;/h3&gt;
&lt;p&gt;接續上次關於 &lt;strong&gt;Tools&lt;/strong&gt; 的討論，當我們賦予了 Agent 雙手去執行任務後，下一個面臨的問題就是「等待」。LLM 生成回應需要時間，尤其是當任務涉及多個工具調用時，漫長的空白等待會毀掉使用者體驗。因此，本篇要來聊聊 LangChain 的 &lt;strong&gt;Streaming (串流)&lt;/strong&gt; 系統，這是提升應用程式響應速度與使用者體驗 (UX) 的核心技術。&lt;/p&gt;
&lt;h3 id=&#34;一-為什麼需要串流&#34;&gt;一、 為什麼需要串流？&lt;/h3&gt;
&lt;p&gt;在 LLM 的應用中，延遲 (Latency) 是不可避免的。串流技術允許我們在完整回應準備好之前，就先將中間過程與部分內容「流」回前端。這不僅能讓使用者感覺系統在即時運作，也能即時顯示 Agent 的思考過程，讓整體互動更加透明、流暢。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;二-langchain-串流的三大核心模式&#34;&gt;二、 LangChain 串流的三大核心模式&lt;/h3&gt;
&lt;p&gt;LangChain 提供了一個靈活的串流系統，主要透過 &lt;code&gt;stream&lt;/code&gt; 或 &lt;code&gt;astream&lt;/code&gt; 方法並配合 &lt;code&gt;stream_mode&lt;/code&gt; 參數來實現：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;模式 (&lt;code&gt;mode&lt;/code&gt;)&lt;/th&gt;
          &lt;th&gt;描述&lt;/th&gt;
          &lt;th&gt;應用場景&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;&lt;code&gt;updates&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;每個 Agent 步驟結束後串流狀態更新。&lt;/td&gt;
          &lt;td&gt;顯示 Agent 目前跑到了哪個節點（如：模型、工具）。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;&lt;code&gt;messages&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;串流 LLM 生成的每一個 Token 與其元數據。&lt;/td&gt;
          &lt;td&gt;實現「打字機」效果或串流工具調用參數。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;&lt;code&gt;custom&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;透過 &lt;code&gt;stream_writer&lt;/code&gt; 從節點內部發送自定義訊號。&lt;/td&gt;
          &lt;td&gt;顯示「正在查詢資料庫 (10/100)&amp;hellip;」等進度訊息。&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h3 id=&#34;三-監控任務進度updates-模式&#34;&gt;三、 監控任務進度：&lt;code&gt;updates&lt;/code&gt; 模式&lt;/h3&gt;
&lt;p&gt;如果你想監控 Agent 的整體執行流程，&lt;code&gt;updates&lt;/code&gt; 是最直觀的模式。它會在每個節點（Node）執行完畢後，噴出該步驟產生的狀態變化。&lt;/p&gt;</description>
    </item>
    <item>
      <title>【Temperature 1.5 的日常】EP3: LangChain - Tools, 賦予 AI 雙手的魔法</title>
      <link>https://aura.codex.tw/posts/temp1.5/ep3/</link>
      <pubDate>Sun, 19 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://aura.codex.tw/posts/temp1.5/ep3/</guid>
      <description>&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;本文為個人學習筆記，記錄了學習過程中的一些知識，可參考，但不可認真，學習的過程可能有理解錯誤，資訊不一定正確，畢竟 Temperature 都 1.5 了。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;零-前言&#34;&gt;零、 前言&lt;/h3&gt;
&lt;p&gt;接續上次關於 &lt;strong&gt;Structured Output&lt;/strong&gt; 的討論，當 Agent 能夠穩定輸出結構化數據後，下一步就是讓它具備「行動力」。在 LangChain 的世界裡，這被稱為 &lt;strong&gt;Tools (工具)&lt;/strong&gt;。工具不僅擴展了 Agent 的能力邊界——讓它能抓取即時數據、執行程式碼或查詢資料庫，更重要的是，它定義了模型如何與真實世界互動的標準介面。&lt;/p&gt;
&lt;h3 id=&#34;一-工具的本質模型與現實的橋樑&#34;&gt;一、 工具的本質：模型與現實的橋樑&lt;/h3&gt;
&lt;p&gt;在底層邏輯中，Tools 是具備明確 &lt;strong&gt;輸入 (Inputs)&lt;/strong&gt; 與 &lt;strong&gt;輸出 (Outputs)&lt;/strong&gt; 的可調用函數。&lt;/p&gt;
&lt;p&gt;當我們將工具傳遞給 Chat Model 時，模型並不是真的「執行」了程式碼，而是根據對話上下文決定「何時」調用工具以及「提供什麼參數」。這種決策機制讓 Agent 能夠像人類操作儀表板一樣，有目的地選擇工具。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;二-快速上手定義工具的三重境界&#34;&gt;二、 快速上手：定義工具的三重境界&lt;/h3&gt;
&lt;h4 id=&#34;1-基礎定義tool-裝飾器&#34;&gt;1. 基礎定義：&lt;code&gt;@tool&lt;/code&gt; 裝飾器&lt;/h4&gt;
&lt;p&gt;這是最簡單的方式。LangChain 會自動將函數的 &lt;strong&gt;Docstring&lt;/strong&gt; 轉換為工具描述，將 &lt;strong&gt;Type Hints&lt;/strong&gt; 轉換為輸入 Schema。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;langchain.tools&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tool&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nd&#34;&gt;@tool&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;search_database&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;limit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;搜尋客戶資料庫中符合查詢條件的紀錄。&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;找到 &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;limit&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt; 筆關於 &amp;#39;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#39; 的結果&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;em&gt;註：Type Hints 是強制要求的，因為這是模型理解如何傳參的唯一依據。&lt;/em&gt;&lt;/p&gt;
&lt;h4 id=&#34;2-自定義屬性&#34;&gt;2. 自定義屬性&lt;/h4&gt;
&lt;p&gt;有時函數名稱不夠直觀，你可以手動覆蓋名稱與描述，以引導模型做出更準確的判斷：&lt;/p&gt;</description>
    </item>
    <item>
      <title>【Temperature 1.5 的日常】EP2: LangChain - Structured Output 結構化輸出的藝術</title>
      <link>https://aura.codex.tw/posts/temp1.5/ep2/</link>
      <pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://aura.codex.tw/posts/temp1.5/ep2/</guid>
      <description>&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;本文為個人學習筆記，記錄了學習過程中的一些知識，可參考，但不可認真，學習的過程可能有理解錯誤，資訊不一定正確，畢竟 Temperature 都 1.5 了。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;零-前言&#34;&gt;零、 前言&lt;/h3&gt;
&lt;p&gt;接續上次關於 LangChain v1.0 架構重構的討論，本篇將深入探討其核心功能之一：&lt;strong&gt;結構化輸出 (Structured Output)&lt;/strong&gt;。在 Agent 的演進中，如何讓模型不再只是「吐出一段話」，而是「回傳一個物件」，是邁向自動化整合的關鍵一步。&lt;/p&gt;
&lt;h3 id=&#34;一-從通靈到規格化為什麼需要結構化輸出&#34;&gt;一、 從「通靈」到「規格化」：為什麼需要結構化輸出？&lt;/h3&gt;
&lt;p&gt;在早期的 LLM 開發中，獲取特定資訊（如從一段文字中提取姓名、電話）通常依賴於「提示詞工程 + 正則表達式」。這種方式在模型版本更迭或語氣變化時極其脆弱。&lt;/p&gt;
&lt;p&gt;LangChain v1.0 透過 &lt;code&gt;create_agent&lt;/code&gt; 的 &lt;code&gt;response_format&lt;/code&gt; 參數，將此流程標準化。現在，Agent 不再回傳模糊的自然語言，而是直接回傳 &lt;strong&gt;JSON 物件&lt;/strong&gt;、&lt;strong&gt;Pydantic 模型&lt;/strong&gt; 或 &lt;strong&gt;Python Dataclasses&lt;/strong&gt;。這意味著你的程式碼可以直接存取屬性（如 &lt;code&gt;result.name&lt;/code&gt;），而不需要再寫 &lt;code&gt;if &amp;quot;Name:&amp;quot; in response&lt;/code&gt; 這種令人崩潰的邏輯。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;二-雙路徑策略provider-vs-tool-calling&#34;&gt;二、 雙路徑策略：Provider vs. Tool Calling&lt;/h3&gt;
&lt;p&gt;LangChain 根據模型的能力，自動切換兩種不同的達成策略：&lt;/p&gt;
&lt;h4 id=&#34;1-providerstrategy-原生支援&#34;&gt;1. ProviderStrategy (原生支援)&lt;/h4&gt;
&lt;p&gt;當你使用的模型提供商（如 OpenAI, Anthropic, Gemini, Grok）原生支持結構化輸出時，這是最可靠的選擇。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;優勢：&lt;/strong&gt; 供應商在 API 層級強制執行 Schema，幻覺率最低。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;新特性：&lt;/strong&gt; 在 &lt;code&gt;langchain&amp;gt;=1.2&lt;/code&gt; 中支援 &lt;code&gt;strict&lt;/code&gt; 參數，強制模型 100% 遵守 Schema。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自動觸發：&lt;/strong&gt; 只要模型支援，直接傳入 Pydantic 類別給 &lt;code&gt;response_format&lt;/code&gt; 即可。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;2-toolcallingstrategy-工具調用&#34;&gt;2. ToolCallingStrategy (工具調用)&lt;/h4&gt;
&lt;p&gt;對於不支援原生輸出的模型，LangChain 會將「輸出規格」包裝成一個「虛擬工具」。&lt;/p&gt;</description>
    </item>
    <item>
      <title>【Temperature 1.5 的日常】EP1: LangChain - 認識 LangChain v1.0</title>
      <link>https://aura.codex.tw/posts/temp1.5/ep1/</link>
      <pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://aura.codex.tw/posts/temp1.5/ep1/</guid>
      <description>&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;本文為個人學習筆記，記錄了學習過程中的一些知識，可參考，但不可認真，學習的過程可能有理解錯誤，資訊不一定正確，畢竟 Temperature 都 1.5 了。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;一-核心背景解決生產力鴻溝&#34;&gt;一、 核心背景：解決「生產力鴻溝」&lt;/h3&gt;
&lt;p&gt;LangChain 從 v0.3 邁向 v1.0，本質上是從「實驗性工具」轉型為「企業級平台」。過去開發者常遇到的四大痛點在 v1.0 得到了正面回應：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;依賴臃腫：&lt;/strong&gt; 解決了過去單體式結構導致的依賴衝突。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;API 不穩定：&lt;/strong&gt; 正式採用「語義化版本控制 (Semantic Versioning)」，承諾重大變更僅在主版本發生。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;黑盒子抽象：&lt;/strong&gt; 淘汰了難以除錯的舊版 Chains，轉向透明的宣告式語法。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;文件滯後：&lt;/strong&gt; 透過架構標準化，大幅改善了開發文件的指導意義。&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;二-架構層級的全面重構&#34;&gt;二、 架構層級的全面重構&lt;/h3&gt;
&lt;p&gt;v1.0 重新定義了開發 Agent 的標準流程，主要體現在以下三個面向：&lt;/p&gt;
&lt;h4 id=&#34;1-新的-agent-範式create_agent&#34;&gt;1. 新的 Agent 範式：&lt;code&gt;create_agent&lt;/code&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;底層支撐：&lt;/strong&gt; 全面改由 &lt;strong&gt;LangGraph&lt;/strong&gt; 驅動，原生支援持久化與人機協作。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;中介軟體 (Middleware)：&lt;/strong&gt; 引入類似 Web 開發的 Hook 機制（如 &lt;code&gt;beforeModel&lt;/code&gt;, &lt;code&gt;wrapToolCall&lt;/code&gt;）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;情境工程：&lt;/strong&gt; 允許開發者在不破壞核心邏輯的情況下，靈活插入 PII 脫敏或自動摘要等功能。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;2-標準化通訊結構content_blocks&#34;&gt;2. 標準化通訊結構：&lt;code&gt;.content_blocks&lt;/code&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;供應商無關：&lt;/strong&gt; 統一了不同模型（OpenAI, Anthropic 等）的輸出格式。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;多模態支援：&lt;/strong&gt; 為未來視覺與檔案內容的處理提供了標準接口。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;類型安全：&lt;/strong&gt; 提供完整的 Type Hints，減少執行時錯誤。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;3-解耦的生態系統&#34;&gt;3. 解耦的生態系統&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;langchain-core&lt;/code&gt;: 穩定的基礎抽象（Runnable 接口）。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;langchain-community&lt;/code&gt;: 獨立的版本控制，處理第三方整合。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;langchain-classic&lt;/code&gt;: 專為舊版功能（如 LLMChain）提供的過渡包，確保升級不中斷。&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;三-組合範式的轉移從-chains-到-lcel&#34;&gt;三、 組合範式的轉移：從 Chains 到 LCEL&lt;/h3&gt;
&lt;p&gt;這是 v1.0 最具影響力的技術變革，將「指令式」轉向「宣告式」：&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
